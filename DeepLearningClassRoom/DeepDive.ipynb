{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Classroom\n",
    "Contact and Copyright: Mark.schutera@mailbox.com\n",
    "\n",
    "\n",
    "\n",
    "I am happy to have you. In the following you will interactively experience and explore the world of artificial intelligence, machine learning or even more specific, deep learning. You will get to know..\n",
    "- the general design of a deep learning pipeline\n",
    "- the basic principles behind deep learning \n",
    "- the fundamental neural network architecture\n",
    "- the process of training a neural network\n",
    "- the deployment of your trained neural network\n",
    "\n",
    "After this course you will know the basics of..\n",
    "- the programming language python\n",
    "- the deep learning packages tensorflow and keras\n",
    "- the mathematics and approaches within deep learning\n",
    "- many more\n",
    "\n",
    "This course will enable you to set up your own deep learning projects, train your own models on your own data, and you will be well on your way to become a deep learning engineer, machine learning researcher, deploying the power of deep learning to generate progress, make the world a better place and solve the pressing problems of today and tomorrow. This sounds exciting - it is! And you are exactly in the right place - Buckle Up!\n",
    "\n",
    "<img src=\"graphics/welcome.svg\" width=\"500\"><br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be smileys along the way, these are for virtual lectures with this notebook. They help your lecturer stay aware on how people progress. Once you reach a smiley, please post it within the chat function of your communication software.\n",
    "\n",
    "üòç Smiling Face with Heart-Eyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Getting comfortable in the driver's seat!\n",
    "\n",
    "The interface you are seeing in front of you is called jupyter notebook, a nice interactive way to present and communicate code. Think of it as your cockpit. The nice thing is that in the back, we already got your engine preheated and ready to rumble, waiting for you to kick the pedal to the metal. \n",
    "\n",
    "In principal this notebook is structured in cells, which get highlighted once you select them. There are two types of cells, the markdown cells (such as this one), are for everything that is not code. Here you will find, explanations, descriptions, figures and many more. The other type of cell is for code, here you can interactively change code and run it within your browser. \n",
    "\n",
    "Navigate to the next cell by using the down-arrow or click on the next cell ..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Great job. This is a python interactive cell with an easy print function. Feel free to change and play around with it.\n",
    "\n",
    "print('Ready Player One!')\n",
    "\n",
    "# A hashtag is used to comment out stuff that is not executed as code. This is useful if you want to add a comment to your code.\n",
    "# Run a cell by clicking the Run button above, or by Ctrl + Enter (on your german keyboard this is Strg + Enter) \n",
    "\n",
    "# Notice the output below the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use this cell to import all the packages you will need in the following - think of it as turning on all your systems\n",
    "# in your cockpit\n",
    "\n",
    "# This makes sure that if you change code in your external scripts, they will be updated\n",
    "\n",
    "\n",
    "import checker\n",
    "import generator\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import importlib\n",
    "importlib.reload(checker)\n",
    "importlib.reload(generator)\n",
    "\n",
    "\n",
    "# now go ahead and Run the cell, this might take a while ..\n",
    "# while the cell is running you will see ln[*] next to it, once it finished you will see the number of the execution\n",
    "# In case you want to interrupt the run of a cell press Ctrl + C (on your german keyboard this is Strg + C) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "Below code cells you sometimes find, what we call hints. So in case you get stuck, or are in search of additional information this is the place for additional elaboration on the topic or links. <br>\n",
    "- [print()](https://docs.python.org/3/library/functions.html#print) <br>\n",
    "- [import](https://docs.python.org/3/reference/simple_stmts.html#import)\n",
    "\n",
    "##### Side Quest:\n",
    "Another feature are the side quests. These are things you could try, think about and tease your brain with. They are not necessary for you to learn the fundamentals, they are nevertheless valuable for your holistic understanding of the subject. Also they are fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Making sure you are still on track\n",
    "\n",
    "Next, welcome to the world of what we call sanity checks.\n",
    "Every now and then you will find sanity checks below your interactive code cells. \n",
    "These are to make sure that your code does what it should (this is usually called a [unit test](https://en.wikipedia.org/wiki/Unit_testing)).\n",
    "Once your code passed the unit test, it is save for you to go ahead with the notebook.\n",
    "So do not touch the code within a sanity check cell, instead just run it, and fix your code in the respective cell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write a function that adds a to b and returns c\n",
    "\n",
    "def add(a,b):\n",
    "    c = # '''your code goes here''' \n",
    "    # the code goes here flag has to be removed, and indicates that you need to pick your brain and write some code.\n",
    "    \n",
    "    return c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "An addition in python can be done by simply writing a '+' sign connecting the two input arguments. An alternative is to use a dedicated function that takes an array of values. <br>\n",
    "- [sum([a, b])](https://docs.python.org/3/library/functions.html#sum) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After writing the add function in the cell above, go ahead and Run this cell.\n",
    "\n",
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_add(add)\n",
    "\n",
    "\n",
    "# In case your code is working properly, you will be prompted with the message \"Everything passed, you are ready to go.\"\n",
    "# This means you can continue with the next cell\n",
    "\n",
    "# In case there is a bug in your code, you will be prompted with an AssertionError, outlining how your code failed.\n",
    "\n",
    "# Go ahead break your code and see what's happening to get a feeling for the sanity checks. Then feel free to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to drive this thing, you are ready for some real action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Talking about brains\n",
    "\n",
    "Your brain is the most astonishing multi purpose computer around. And neuroscientists and engineers alike have been trying to understand the mechanics of brains, to apply their findings to other fields, such as computer science. This is how the idea of artificial neural networks came into being. <br>\n",
    "\n",
    "Your brain is made up of approximately 80 billion neurons, or brain cells, which are heavily interconnected with what are called synapses. Neurons communicate with each other by sending electrical impulses through these synapses. If a neuron receives enough of these messages, the neuron itself sends an impulse to the neurons it is connected with. In this way the brain is able to process complex data, extract features and recognize pattern. <br>\n",
    "\n",
    "And this is not yet, the most exciting ability of the brain. Your brain is further able to learn, adjusting to newly perceived data, pattern and behavior. This happens by either strenghtening or weakening the connection or in other words synapses between neurons, influencing their interplay. <br>\n",
    "\n",
    "\n",
    "<img src=\"graphics/first_pot.svg\" width=\"700\"><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üôÉ Upside-Down Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A computational model\n",
    "\n",
    "As of today the brain is not fully understood, yet the very idea behind neural networks can be mold into a mathematical and computational model. For the purpose of explainability we further reduce our neural network to a single neuron with two inputs $x_0$ and $x_1$, that travel along synapses with connection factors $w_0$ and $w_1$. The neuron then sends an output $y$ with respect to the activation function $f($**x**$, $**W**$)$. <br>\n",
    "\n",
    "To make things easier for our silicon based computers, it is usually assumed that inputs are send at the same time and that there is only one direction in which the information flow (meaning no loops within the network). In deep learning speach we refer to this as [feed forward](https://en.wikipedia.org/wiki/Feedforward_neural_network). <br>\n",
    "\n",
    "To understand the principle concept you will now go ahead and build this simplified model of a neuron. We will then teach this neuron to model an underlying function $(a * x_1 + b * x_2)^2 = y$. We would thus expect the weights to converge onto a and b. <br>\n",
    "\n",
    "#### Set up your neuron for a first run\n",
    "Define an input vector **x** with two input values and a fully connected weight matrix **W** with two synapses. The groundtruth should be defined as 1.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\textbf{x} =\\begin{bmatrix}\n",
    "\t0.2 \\\\ 0.4 \n",
    "\t\\end{bmatrix}\n",
    "\\end{align}\t\n",
    "\n",
    "\\begin{align}\n",
    "\t\\textbf{W} =\\begin{bmatrix}\n",
    "\t0.1 \\\\ 0.5 \n",
    "\t\\end{bmatrix}\n",
    "\\end{align}\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting up the inputs and the weight matrix \n",
    "# (do not forget to run code cells that have entries which are used in later cells)\n",
    "\n",
    "x = np.array([[0.2], [4]])\n",
    "W = np.array([[0.1], [0.5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to write functions which (1) determine the strength of the signals to our neuron and (2) that create the appropriate and accumulated output signal based on the signal accumulation. This split into two functions is done to make our next step easier, but for now go ahead and implement them.\n",
    "\n",
    "\\begin{align}\n",
    "\t f_{(1)}(\\textbf{x,W})=  \\textbf{W*x}, \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\t f_{(2)}(\\textbf{x,W})= ||f_{(1)}||^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating the strength of the signals to the neuron\n",
    "def accumulation(x,W):\n",
    "    q = #'''your code goes here'''\n",
    "\n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "For this step the dot product could be used. The numpy package which we already imported as np provides such functionality. <br>\n",
    "- [np.dot(a, b)](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating the activation of the neuron\n",
    "def activation(q): \n",
    "    y = #'''your code goes here'''\n",
    "   \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "The output after the activation (squaring the inputs) needs to be accumulated or summed up. Squaring in this case is easiest with the multiplication operator *. <br>\n",
    "- [np.sum()](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) <br>\n",
    "\n",
    "##### Side Quest:\n",
    "Feel free to play around with the activation function, what type of activation function might be more meaningful. Especially when thinking of how a neuron reacts only after a certain threshold is reached, and how it is limited when it comes to the output strength. \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let me put the things together for you (if the sanity checker breaks, check your accumulation and activation code), \n",
    "# allowing you to see your neuron in action for the first time.\n",
    "# This is what we call a forward pass.\n",
    "def forwardPass(x, W):\n",
    "    q = accumulation(x, W)\n",
    "    y = activation(q)\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_forwardPass(forwardPass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make your neuron learn \n",
    "The forward pass, is basically what you want your neuron, and your whole neural network to do. But only once it is trained.\n",
    "Meaning first the weights and biases have to be determined in a way such that the neuron outputs what we would expect it to output.\n",
    "\n",
    "Let's remember our task, we want the model to model an underlying function.\n",
    "\n",
    "This model behavior is learned by iteratively adjusting the weights of the neural network, such that the current prediction gets closer to the expected output, which is available as a label or ground truth $\\tilde{\\textbf{y}} = (a * x_1 + b * x_2)^2$.\n",
    "\n",
    "Now that we know the general idea of lets implement the training procedure step by step.\n",
    "Lets start at the bottom of the neural network, at the output layer, or more precise the prediction $\\hat{\\textbf{y}}$.\n",
    "To begin we need to get to know how far our neural network was actually off.\n",
    "\n",
    "\n",
    "#### Objective function and Loss\n",
    "The offset of a neural network is determined by an objective function, which then yields a loss. The design of an objective function is crucial for the training process, as the weight parameters converge faster if the objective function models the error well. This said, a less suitable, yet correct objective function will still take you to the finish line. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# implement the mean squared error as objective function\n",
    "def objectiveFunction(y_pred, y):\n",
    "    loss = #'''your code goes here'''\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "In our implementation, the loss is going to be calculated for a single sample a time. Numpy has your back with a square function <br>\n",
    "- [np.square()](https://numpy.org/doc/stable/reference/generated/numpy.square.html) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_objectiveFunction(objectiveFunction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "Now that we know about the loss, we need to update the weights and parameters in a way that next time the neural network sees the same input it is a little bit less off. This update step is called backpropagation and is one of the core concepts of deep learning and neural networks.\n",
    "\n",
    "[Backpropagation](https://cs231n.github.io/optimization-2/) computes the gradients of the nested operations of your neural network through recursive application of chain rule. Starting from the calculated loss, this allows to trace back which weight had which share of the error, and more importantly in which direction the weight has to be adjusted in order to reduce the error. \n",
    "\n",
    "So let's determine the partial derivatives of our neuron. We aim to determine the gradient of our function \n",
    "$f(\\textbf{x,W})$ dependent on the input values of our network $\\textbf{x}$, and the network's parameters $\\textbf{W}$.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{\\partial f(\\textbf{x,W})}{\\partial \\textbf{W}}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(\\textbf{x,W})}{\\partial \\textbf{x}}\n",
    "\\end{align}\n",
    "\n",
    "As we can not influence the input in order to reduce the loss, this partial derivative with respect to $\\textbf{x}$ is waved.\n",
    "By applying the chain rule we can now propagate the error through the local derivatives at each operation and neuron. Starting from the output layer, working our way up to the input layer.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{\\partial f(\\textbf{x,W})}{\\partial \\textbf{W}} = \\frac{\\partial f(\\textbf{q})}{\\partial \\textbf{q}} * \\frac{\\partial \\textbf{q}}{\\partial \\textbf{W}} \n",
    "\\end{align}\n",
    "\n",
    "With\n",
    "\\begin{align}\n",
    "\tf(\\textbf{q})= ||\\textbf{q}||^2 = q_{1}^2 + q_{2}^2,\n",
    "\\end{align}\n",
    "\n",
    "we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(\\textbf{q})}{\\partial \\textbf{q}} = 2 * q_{i}.\n",
    "\\end{align}\n",
    "\n",
    "Great, we have the first part, now let's concentrate on the part before the activation function where\n",
    "\n",
    "\\begin{align}\n",
    "\t\\textbf{q} = \\textbf{W} * \\textbf{x},\n",
    "\\end{align}\n",
    "\n",
    "and thus \n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial q_{i}}{\\partial \\textbf{W}} = \\textbf{x}.\n",
    "\\end{align}\n",
    "\n",
    "Putting things together we get the gradients with respect to the weights $\\textbf{W}$\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(\\textbf{x},\\textbf{W})}{\\partial \\textbf{W}} = 2 * q_i * \\textbf{x}^{T} * 2 * (\\hat{\\textbf{y}} - \\tilde{\\textbf{y}})\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implement the gradientFunction with respect to the weights W (see the above cell)\n",
    "def gradientFunction (q, x, y_pred, y):\n",
    "    deltaW = # '''your code goes here'''\n",
    "    \n",
    "    return deltaW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "You will need the transposed of an array. <br>\n",
    "- [np.ndarray.T](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_gradientFunction(gradientFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The weight update\n",
    "Now that we know from the gradient in which direction we need to change the weights, we are going to do just that.\n",
    "The direction put aside, we need to specify how much the weights are going to be changed, in optimization this is usually called step size. In neural network training this is called the learning rate.\n",
    "This is arguably the most important parameter and it is pretty tricky to set: Too small slows down the convergence, too large speeds up convergence, but is risky as this might prevent the training from full convergence. \n",
    "\n",
    "\\begin{align}\n",
    "    \\textbf{W}_{t+1} = \\textbf{W}_{t} - learningRate * \\partial \\textbf{W}_{t}.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update the weights, control the magnitude of the change with the learning rate parameter. \n",
    "def update (W, deltaW, learningRate):\n",
    "    W = # '''your code goes here'''  \n",
    "    \n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_update(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the things together for the training of your first neural network\n",
    "We are getting excitingly close to our first neural network training. The underlying model your neural network is going to depict is $\\tilde{\\textbf{y}} = (a * x_1 + b * x_2)^2$. The convenient thing here is, that our neural network architecture is pretty much the same as this function, such that $w_1$ converges to the value of $a$ and $w_2$ to $b$. As we are trying to understand the workings and mechanics within a neural network, this allows for easier tracing and interpretability while training the network. \n",
    "\n",
    "Of course in practice the underlying models and pattern will be way more difficult, but starting small allows us to make a first step in a safe environment. \n",
    "\n",
    "Remember you can stop a cell from executing with the squared stop buttom above and rerun it with the triangle Run button.\n",
    "\n",
    "\n",
    "<img src=\"graphics/machine.svg\" width=\"700\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòâ Winking Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning rate\n",
    "learningRate = 1e-5\n",
    "\n",
    "# Weight initialization\n",
    "W = np.array([[0.1, 0.8]])\n",
    "\n",
    "# Define target parameters of your function\n",
    "a = 0.5\n",
    "b = 0.6\n",
    "\n",
    "\n",
    "for updates in range(1, 1000):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # This is where we use a preimplemented generator function to generate a new dataset pair \n",
    "    x1, x2, y = generator.generateDataPair(a, b)\n",
    "    x = np.array([[x1], [x2]])\n",
    "    display('This is update step: ' + str(updates))\n",
    "    display('Target weights are: [[' + str(a) + ' ' + str(b) + ']]')\n",
    "    display('Current weights are: ' + str(W))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Forward pass\n",
    "    y_pred = #'''your code goes here'''\n",
    "    \n",
    "    \n",
    "    # Calculate the loss of your neural network's prediction\n",
    "    loss = #'''your code goes here'''\n",
    "    display(\"Training loss: \" + str(np.round(loss, 2)))\n",
    "    \n",
    "    \n",
    "    # Backpropagation to calculate the gradient\n",
    "    q = #'''your code goes here'''\n",
    "    deltaW = #'''your code goes here'''\n",
    "    \n",
    "    \n",
    "    # Update the weights of your neural network\n",
    "    W = #'''your code goes here'''\n",
    "\n",
    "    \n",
    "    \n",
    "    display(\"Current prediction: \" +  str(np.round(y_pred, 2)) + '  (' + str(np.round(y, 2)) + ')')\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "In this case, there won't be a sanity check, as you will find out whether your network works if your current weights converge onto your target weights (which are the same as $a$ and $b$. <br>\n",
    "\n",
    "\n",
    "##### Side Quest:\n",
    "Feel free to play around with your neural network, and try to answer the questions below to get a feeling for the mechanics of the training.\n",
    "Great places to start are:\n",
    "- The learning rate (What happens for learning rates that are too large or very small? [Hint - Exploding Gradients](https://www.deeplearningbook.org/contents/optimization.html))\n",
    "- The weight initialization (What happens if the initialized weights are the same as the target parameters? [Hint - Parameter Initialization](https://www.deeplearningbook.org/contents/optimization.html))\n",
    "- Once you worked your way through the rest of the notebook, feel free to come back. And try to implement another activation function, network architecture, and adjust the modules above to suit your design purpose.\n",
    "- What impact does the amount of update steps have on the result?\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learnt\n",
    "\n",
    "Your neural network has learnt to depict an underlying function. \n",
    "You have learnt how to set up a neural network training: Ranging from forward pass, to back propagation. Without actively noticing you touched on even more of the fundamental principals of deep learning and its nuts and bolts.\n",
    "\n",
    "Nevertheless, this was quite some work to get a single neuron to learn a rather disentchanting function, wasn't it?\n",
    "\n",
    "Luckily there are python packages that happen to do all of the brain teazing and gradient crushing for us. Below we are going to import the most important ones, and check their version, making sure you have the right ones. In case you do not have the expected version update your packages in the anaconda prompt like this:\n",
    "\n",
    "<img src=\"graphics/tryCoffee.svg\" width=\"500\"><br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --upgrade tensorflow==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get to concentrate on the training hyperparameters and process and the neural network architecture design. Not to forget adding layers - juhuuu!\n",
    "\n",
    "<br>\n",
    "\n",
    "In the following you will be introduced to ..\n",
    "- .. [tensorflow](https://www.tensorflow.org/) \n",
    "- .. [keras](https://keras.io/)\n",
    "\n",
    "While you will clamber on the \"Hello World!\" of deep learning, the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is this joke, that once you have added this import line to your code, \n",
    "# you are allowed to sell your product telling everyone that you are using AI. Life is easy sometimes.\n",
    "import tensorflow as tf        \n",
    "print('Tensorflow version:', tf.__version__, '(Expected 2.4.0)')\n",
    "\n",
    "\n",
    "\n",
    "# Keras is a model-level library, meaning that it is built upon tensorflow (using it as a backend) - allowing for\n",
    "# high-level building blocks. Making it even easier to design neural networks.\n",
    "# We will call it as tf.keras\n",
    "\n",
    "# The tf and k abbreviations are best practice (same for numpy np and pandas pd), \n",
    "# since you do not want to type T E N S O R F L O W all over your code.\n",
    "# They are prevalent all over the industry and academia, in a way that you'll risk a fight if you import them differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hint: \n",
    "The [space versus tabs](https://www.youtube.com/watch?v=SsoOG6ZeyUI&feature=emb_logo) war is a birthday party in comparison to breaking import guidelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the MNIST dataset\n",
    "\n",
    "Before any data science or machine learning project it is most important to get to know your data. This will enable you to detect, issues, noise, pitfalls, and to understand what your model will be able to learn at the end of the day. With the MNIST dataset you will be working with a beautiful, cleaned, easy to understand, low-memory, large-scale dataset. \n",
    "Full disclaimer, this is great for learning and research purposes, yet this is not what you can expect in real-life. \n",
    "\n",
    "This initially becomes obvious as MNIST is so commonly used, that the keras packages got our back with loading the data in one line: We are loading pairs of samples and ground truth annotations for both the training set (for training our model) and the test set (for testing our model). \n",
    "\n",
    "<img src=\"graphics/jars.svg\" width=\"650\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòÖ Grinning Face with Sweat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset in one line\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Printing the shape\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_test:', x_test.shape)\n",
    "print('y_test:', y_test.shape)\n",
    "\n",
    "# Plotting data samples\n",
    "print('\\n Plot of the first 25 samples in the MNIST training set')\n",
    "numbers_to_display = 25\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(numbers_to_display):\n",
    "    plt.subplot(num_cells, num_cells, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the data arrays already gave away that we are using way more samples (60k) for training than for testing (10k). A common ratio is 80% training and 20% testing as a rule of thumb. Actually the 80% training is further splitted into the real training and a validation set, but that will not bother us at here. \n",
    "\n",
    "The other thing the shape gave away is the dimensions of the samples $28x28x1$ and the dimension of the ground truth $1$.\n",
    "\n",
    "Along with the shape we also did plot the first 25 sample of the MNIST training set together with their ground truth labels (annotations) and we can start to guess what the dataset is all about - it is a dataset purposed for digit recognition.\n",
    "\n",
    "\n",
    "### Data preprocessing\n",
    "We already talked quite a bit about [activation functions](https://cs231n.github.io/neural-networks-1/) (or non-linearity) and how they  take a single, usually accumulated, number and perform a certain fixed mathematical operation on it. There are quite different ones, but you will soon find out that within deep neural networks there emerges a shared pattern which makes them beneficial in practice.\n",
    "\n",
    "- They usually map any incoming value in a way that they are asymptotically bounded from above and below. For example, the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) does map to a range of values between $0$ and $1$.\n",
    "- They are usually most sensitive around zero, because for large numbers they tend to saturate to the bounds of the mapping.\n",
    "\n",
    "Thus we can speed up the training process, by preprocessing our data so that the activation function of the first layer has an easier time picking up the pattern.\n",
    "\n",
    "For the MNIST this is easily done by normalizing the grayscale values $[0, 255]$ to a range between $[0, 1]$. Remember to do the preprocessing for both the training and the test samples.\n",
    "\n",
    "\n",
    "<img src=\"graphics/standardizedBeans.svg\" width=\"700\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_normalized = #'''your code goes here'''  \n",
    "x_test_normalized = #'''your code goes here'''  \n",
    "\n",
    "# in a next step we also need to reshape our input so that it will fit our input layer later on. \n",
    "# This is due to keras expecting a definition for how many channels your input sample has, as we \n",
    "# deal with gray scale this is 1.\n",
    "x_train= x_train_normalized.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test_normalized.reshape(-1, 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " ____   __   __ _  __  ____  _  _     ___  _  _  ____  ___  __ _ \n",
    "/ ___) / _\\ (  ( \\(  )(_  _)( \\/ )   / __)/ )( \\(  __)/ __)(  / )\n",
    "\\___ \\/    \\/    / )(   )(   )  /   ( (__ ) __ ( ) _)( (__  )  ( \n",
    "(____/\\_/\\_/\\_)__)(__) (__) (__/     \\___)\\_)(_/(____)\\___)(__\\_)\n",
    "\n",
    "Do not touch, just run!\n",
    "'''\n",
    "checker.test_normalize(x_train, x_train_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "In python a division can be done like this in python:  $3, 5~/~ 2 == 2.5$ <br>\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the neural network architecture\n",
    "\n",
    "Remember how much work it was to set up a single neuron - good. Now enjoy how Keras is helping you with this. We will call our neural network [marvin](https://en.wikipedia.org/wiki/Marvin_the_Paranoid_Android), feel free to change it to whatever you like, in case you are not okay with it.\n",
    "\n",
    "Our model will follow some design requirements, it will be [sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential?version=stable) or it other words feed forward, meaning that once a value passed through a unit, the unit will not see a value during that forward pass again. Or in other words we do not have any recurrence in the architecture (feedback loops and so on). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚ÄúDon't blame you,\" said Marvin and counted five hundred and ninety-seven thousand million sheep \n",
    "# before falling asleep again a second later.‚Äù\n",
    "marvin = tf.keras.models.Sequential()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now adding layers to marvin is as easy as adding a layer, while handing over the specification of that layer. The layers will in the end be stacked following the order in which your code gets compiled.\n",
    "\n",
    "Initially we need an input layer, which is able to take in our input samples. Go ahead and set up a 2D convolutional layer, with an input shape fitting our image samples, with a kernel size of 5, with 8 channels or filters, a stride of 1, and an activation function that is an identity mapping to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marvin = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "# Design the input layer\n",
    "marvin.add(tf.keras.layers.Convolution2D( #'''your layer type goes here'''  \n",
    "    input_shape=(28, 28, 1), #'''your input shape goes here'''  \n",
    "    kernel_size=5, #'''your kernel size goes here'''  \n",
    "    filters=8, #'''your number of filters goes here'''  \n",
    "    strides=1, #'''your stride goes here'''  \n",
    "    activation=tf.keras.activations.relu #'''your activation function goes here'''  \n",
    "))\n",
    "\n",
    "\n",
    "# Add your hidden layers here.\n",
    "# ''' Your hidden layer goes here '''\n",
    "\n",
    "\n",
    "# To prepare the outputs of your last hidden layer up your model you will need to \n",
    "# flatten all its outputs\n",
    "marvin.add(tf.keras.layers.Flatten())\n",
    "\n",
    "\n",
    "# Then it is about time to output your prediction, remember that there are 10 classes in our\n",
    "# MNIST digit classification problem\n",
    "marvin.add(tf.keras.layers.Dense(\n",
    "    units= # ''' Your code goes here''',\n",
    "    activation= # ''' Your code goes here''',\n",
    "))\n",
    "\n",
    "\n",
    "marvin.summary()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "Find out about keras activation functions [here](https://keras.io/api/layers/activations/) and about layers [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers). The input shape only needs to be defined for the input layer. Using softmax in your output layer will give you the pleasant effect of a probability distribution for your prediction. The output of softmax transformation has all values non-negative and sum to 1.\n",
    "\n",
    "<br>    \n",
    "\n",
    "Keep on designing your model until your model summary is the same as this one:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the neural network training\n",
    "\n",
    "Great that's it, take a moment to realize how quickly and with how much ease (maybe not during the first time) you designed this architecture with $67k$ parameters. Also within the blink of an eye you created a neural network that has more neurons than a box jellyfish ($17.5k$ neurons), a sea slug ($18k$), coming in behind the fruitfly ($250k$). To put things into perspective humans come in second with ($8.6\\times10^{10}$) neurons, only surpassed by the african elephant. Find a full list [here](https://en.wikipedia.org/wiki/List_of_animals_by_number_of_neurons). \n",
    "\n",
    "Next we need to train marvin to be able to assign a class prediction when we hand him over a sample of a digit. \n",
    "\n",
    "Remember when we set up our loss function and learning rate? That's exactly what we are going to do here as well, yet again with the help of keras.\n",
    "\n",
    "\n",
    "<img src=\"graphics/blackCoffee.svg\" width=\"500\"><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define your learning rate and optimizer\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "\n",
    "marvin.compile(\n",
    "    optimizer=sgd,\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "Make sure you understand the difference between a [metric](https://keras.io/api/metrics/) (only tells you how good your neural network performs) and a [loss](https://keras.io/api/losses/) (tells you how your model performance, but also directly tells your model how to update its weights to improve performance).  <br>\n",
    "\n",
    "\n",
    "##### Side Quest:\n",
    "Feel free to play around with your neural network and the learning configuration.\n",
    "Great places to start are:\n",
    "- The [optimizer](https://keras.io/api/optimizers/), the losses, the metrics.\n",
    "- Adding layers, changing Kernel sized, strides, and activation functions.\n",
    "- You might have guessed, the learning rate, truth be told playing around with each hyperparameter will get you some experience and an intuition for its behavior. \n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now it is time to plug our data into marvin and let him learn to recognize the digit within an image sample. Besides plugging in the data, and the labels, we need to let keras know how long we want to train marvin. In deep learning training on each sample of the training set once, is called an epoch.\n",
    "Obviously the longer you train the better your model will fit the underlying pattern. Then you can also set how many samples marvin will see during each update step. The more samples the better the approximation of the gradient, but at some point you will also run in hardware limitations.\n",
    "\n",
    "Yet, remember that it is the unseen data of your validation set that machine learning approaches are after. This is also what lets deep learning stand out from pure optimization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get ready for your first training run.\n",
    "marvin.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Hint: \n",
    "Even if you stop the training, the current state of the model will be saved as long as your juypter notebook session is active. Also there is more to specify for training within the fit function of keras. Check it our [here](https://keras.io/api/models/model_training_apis/) <br>\n",
    "\n",
    "##### Side Quest:\n",
    "Reach at least a validation performance of $95 \\%$ or higher before moving on. To achieve this you are free to change whatever you want within the architecture and training process. Obviously a better performance is a great achievement.\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# after the training finishes we will also save marvin in keras style (HDF5) so we do not have to \n",
    "# train him again\n",
    "# everytime we start our computer. Obviously by changing the model_name you can also save different\n",
    "# configurations of marvin. The name has to be a string, like this: 'name'\n",
    "model_name = # ''' Your model name goes here'''\n",
    "marvin.save(model_name, save_format='h5')\n",
    "\n",
    "# It is best practise to indicate what configuration changes you did within the name so you know\n",
    "# which model you need to load already from its name\n",
    "# Let's say instead of a learning rate of 0.001 you used 0.1, your naming could then look like:\n",
    "# 'marvin_lr01.h5'\n",
    "\n",
    "print('Success! You saved marvin as: ', model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluating Marvin\n",
    "\n",
    "Yes, you are right, the job is basically done. Marvin has been trained and you also made sure that the model recognizes more than $95 \\%$ of the unseen samples correctly. So would you say, that you know what marvin learned or what exactly he is capable of in the end?\n",
    "\n",
    "#### Side Quest:\n",
    "Try to answer the following questions:\n",
    "\n",
    "- Which digit is hardest for marvin to predict correctly?\n",
    "- Which two digits does marvin confuse most often?\n",
    "- Which digit does he perform best in?\n",
    "- How do digits look that marvin was not able to predict correctly? Can you see why?\n",
    "\n",
    "These questions are hard to answer and make it clear that an in depth analysis of the models performance is paramount. I personally like a mix of visualization (which helps my intuition) and number crushing (which helps to write nice reports and actually proof my intuition with numbers).\n",
    "\n",
    "So in the following we will go ahead and answer some of the aforementioned questions. Along the way you will learn some strategies to evaluate your neural network.\n",
    "\n",
    "\n",
    "<img src=\"graphics/stars.svg\" width=\"500\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòÜ Grinning Squinting Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Intuitive Approach\n",
    "\n",
    "We will start with the intuition building approach. The following code will plot the image samples, the caption below \n",
    "is marvin's prediction. If he is right, the sample will get a greenish touch, if he is wrong a redish one. Try to answer the above questions with the results from the intuitive plot you will generate below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load a saved marvin configuration you want to evaluate\n",
    "model_name = 'marvin.h5'\n",
    "marvin_reloaded = tf.keras.models.load_model(model_name)\n",
    "\n",
    "# Let marvin predict on the test set so we have some data to evaluate his performance.\n",
    "predictions = marvin_reloaded.predict([x_test])\n",
    "\n",
    "# Remember that the prediction of marvin is a probability distribution over all ten digit classes\n",
    "# We want him to assign the digit class with the highest probability to the sample.\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "#pd.DataFrame(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot for the intuitive approach\n",
    "\n",
    "numbers_to_display = 196\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for plot_index in range(numbers_to_display):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    color_map = 'Greens' if predicted_label == y_test[plot_index] else 'Reds'\n",
    "    plt.subplot(num_cells, num_cells, plot_index + 1)\n",
    "    plt.imshow(x_test_normalized[plot_index].reshape((28, 28)), cmap=color_map)\n",
    "    plt.xlabel(predicted_label)\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### The Empirical Number Crunshing Approach\n",
    "\n",
    "Now that you intuitively answered the questions, let's do some data analysis on the results and see how much you were off, and where you were right and picked up patterns in Marvin's capabilities. \n",
    "\n",
    "#### Hint: \n",
    "Usually visualizations to build your intuition are most beneficial during your first training runs on problems you do not know what to expect from your neural network. In visualizations you might also be able to pick up wrong label assignments (think somebody labeled all \"8\"s as \"9\"s), ambiguous or overlapping classes (think somebody labeled all \"6\"s and \"9\"s as the same class). These things do not usually happen in beautifully, cleaned datasets such as MNIST. But I promise you, you will be surprised how messy these public datasets out there are. And further I promise you, that if you are building your own datasets these bugs will be there too and want to be found.\n",
    "\n",
    "Now for the analytical approach we are going to look at one of the most basic strategies, the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) or error matrix. This is a table that shows the actual class of the sample (horizontal), plotted over which class it was predicted (vertical). To make things clearer, numbers in the diagonal are true positives, meaning Marvin predicted them correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(y_test, predictions)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "sn.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.7,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts on the side quest questions and experiments\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to say Goodbye\n",
    "Well actually not quite yet. Now as you are capable of training neural networks, learned about the fundamentals and also are able to evaluate your model's performance in a professional manner - I want to point out some things you can look into next along your path to become a machine learning engineer or deep learning researcher.\n",
    "\n",
    "#### In this notebook\n",
    "You can definitely come back to this notebook again and again. Play with the hyperparameters, try to follow through the notebook with other datasets. There are quite a few [directly supported](https://www.tensorflow.org/datasets/catalog/overview) by tensorflow, so they are easy to plug in to this notebook. Or try to load a dataset from disk and write your own data pipeline.\n",
    "\n",
    "Come back and try working on other layers, such as Dropout or Batchnorm. Get to know regularization strategies, such as augmentation and try to implement them here as well. Think of this notebook as your bridge head to the world of deep learning. This is the place to break things and try stuff.\n",
    "\n",
    "#### Literature\n",
    "Of course reading helps to get to know new approaches, different ideas, and to dive deeper and build up your knowledge. I recommend the following books to start with. From there start go with the actual papers.\n",
    "\n",
    "[To start with](http://www.deeplearningbook.org/) Deep Learning Book - Ian Goodfellow and Yoshua Bengio and Aaron Courville\n",
    "Abstract: The Deep Learning textbook is a resource intended to help students and practitioners enter the field of machine learning in general and deep learning in particular. The online version of the book is now complete and will remain available online for free.\n",
    "\n",
    "[To understand the fundamentals](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf) Pattern Recognition and Machine Learning - Bishop Abstract: ThisnewtextbookreÔ¨Çectstheserecentdevelopmentswhileprovidingacomprehensive introduction to the Ô¨Åelds of pattern recognition and machine learning. It is aimed at advanced undergraduates or Ô¨Årst year PhD students, as well as researchers and practitioners, and assumes no previous knowledge of pattern recognition or machinelearningconcepts.\n",
    "\n",
    "[To get practice](https://www.coursera.org/courses?query=andrew%20ng) Deep Learning and Machine Learning Courses on Coursera - Andrew Ng Abstract: In these courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory, but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach.\n",
    "Link: \n",
    "\n",
    "[From beginning to mastery](http://cs231n.stanford.edu/) CS231n: Convolutional Neural Networks for Visual Recognition - Andrej Karpathy Abstract: Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka ‚Äúdeep learning‚Äù) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This course is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. During the 10-week course, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision. The final assignment will involve training a multi-million parameter convolutional neural network and applying it on the largest image classification dataset (ImageNet). We will focus on teaching how to set up the problem of image recognition, the learning algorithms (e.g. backpropagation), practical engineering tricks for training and fine-tuning the networks and guide the students through hands-on assignments and a final course project. Much of the background and materials of this course will be drawn from the ImageNet Challenge. There is also a YouTube [lecture](https://www.youtube.com/watch?v=NfnWJUyUJYU&list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC). \n",
    "\n",
    "\n",
    "#### Mailing list\n",
    "There is so much more out there. Different neural network architecture, different learning paradigms, not to mention endless real-world application possibilities.\n",
    "Everynow and then I'll share another part of this world. If you want to keep posted, write an e-mail to mark.schutera@kit.edu with subject: \"AppliedDeepLearningSchool Newsletter\" - I promise there will be only little spam.\n",
    "\n",
    "#### Someone deleted the internet or a link broke in the notebook? \n",
    "You wanted to know more, clicked a link - and it was dead? That happens, as the internet and especially software is a living thing. In that case I would be very happy if you'd let me know, so that stuff gets fixed for the ones after you. You noticed a bug, a typo, an error, something? Send me an email to mark.schutera@mailbox.org subject: \"AppliedDeepLearningSchool Bug Hunt\".\n",
    "Any other feedback is also highly appreciated.\n",
    "\n",
    "#### Can you use this code for a school or university project?\n",
    "Sure thing, go ahead. Just make sure to cite it appropriately. If you are unsure on how to do this, again feel free to reach out. I can not wait to see you burning through school and university competitions with deep learning applications. \n",
    "\n",
    "\n",
    "<img src=\"graphics/droplet.svg\" width=\"650\"><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üòá Smiling Face with Halo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some space for you to write down your thoughts\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
