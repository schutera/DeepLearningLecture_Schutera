{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "contact: Mark.schutera@kit.edu\n",
    "\n",
    "\n",
    "# Recurrent Neural Networks (RNNs) for Text Generation\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to implement a recurrent neural network for text generation based on a text dataset from Shakespeare / Goethe. The tutorial has been inspired by the original [tensorflow tutorials](www.tensorflow.org) and Andrej Karpathies work and thus based on numpy only. \n",
    "\n",
    "<img src=\"graphics/GoetheSchillerWeimar.jpg\" width=\"700\"><br>\n",
    "<center> Fig. 1: The german poets Goethe and Schiller in Weimar. Image from [pixabay](https://pixabay.com/de/photos/weimar-goethe-schiller-denkmal-806851/) </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Idea \n",
    "\n",
    "In this tutorial we will focus on a text generation approach based on a character-based RNN. We will work with a dataset similar to the dataset of Shakespeare's writing provided by Andrej Karpathy's and first used in his [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Mephist\"), train a model to predict the next character in the sequence (\"o\"). Longer sequences of text can be generated by calling the model repeatedly, or in other words recurrent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Text Data\n",
    "\n",
    "First we need to scrape data. A good source to scrape is [Project Gutenberg](https://www.gutenberg.org/).\n",
    "Since there are legal struggles with project gutenberg we will use Faust 1 by Goethe, scraped from [wikisource](https://de.wikisource.org/wiki/Faust_-_Der_Tragödie_erster_Teil). Or shakespeare already scraped for us by Google.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# Link to Goethe data\n",
    "path_to_file = 'data/Faust1_Goethe.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete unwanted characters\n",
    "import re\n",
    "char_list = '''select characters to clean the dataset'''\n",
    "text = re.sub(\"|\".join(char_list), '''substitute with an empty string''', text)\n",
    "\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vocab)\n",
    "\n",
    "# Look at the first 1000 characters\n",
    "'''print the first 1000 characters of the text'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the characters\n",
    "Before training the strings need to be vectorized, meaning we need to bring them in a numerical representation, for our neural network to be able to work with. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char_to_ix = {u:i for i, u in enumerate(vocab)}\n",
    "ix_to_char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array('''transpose all characters of the text to ints''' for c in text])\n",
    "\n",
    "# Visualize the mapping\n",
    "print('{')\n",
    "for char,_ in zip(char_to_ix, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char_to_ix[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the data\n",
    "The task of the neural network is a prediction task. Given a sequence of characters what is the most probable next character. \n",
    "We can then move our receptive field one stride forward and the neural network again performs a prediction for the next character. The output will be our input shifted one character to the right. \n",
    "\n",
    "For this we use the [from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) utility of tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input dimensions\n",
    "# Make a reasonable choise for the sequence length, elaborate. \n",
    "seq_length = 100\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "\n",
    "    # How is the following pipeline also called? It is the ... of the model.\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing our Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designing our Recurrent Neural Network\n",
    "\n",
    "# Make a reasonable choise for the learning rate, elaborate. \n",
    "learning_rate = 1e-3\n",
    "hidden_size = 100\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = '''design the weight matrix for the hidden layer'''\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
    "by = np.zeros((vocab_size, 1))  # output bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the loss function for the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh)  # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t - 1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "while True:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p + seq_length + 1 >= len(text) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "\n",
    "    inputs = [char_to_ix[ch] for ch in text[p:p + seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in text[p + 1:p + seq_length + 1]]\n",
    "   \n",
    "    # sample from the model now and then\n",
    "    if n % 1000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt,))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))  # print progress\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judging our Text Generator\n",
    "\n",
    "When judging the results keep in mind our prerequisits.\n",
    "Following is an example of 1000 characters of a model trained for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
    "- The model is character-based. When training started, the model did not know how to spell a single word, let alone german word, or that words were even a unit of text.\n",
    "- The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
    "- As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- Write a function to capture the current state of the model, to be able to save and later continue the training. How would you go about it (mail to mark.schutera@kit.edu for a chance to get bonus points). \n",
    "- Scrape Kafka or Goethe and try training on a different dataset. What are your findings - can you distinguish between generated Shakespeare and generated Kafka?\n",
    "- So far our approach is character based, try a word based approach, what are the advantages, and what are the drawbacks? Which approach would you choose with respect to dataset size, computational power, and overall performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
