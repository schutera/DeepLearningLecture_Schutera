{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "contact: Mark.schutera@kit.edu\n",
    "\n",
    "\n",
    "# Deep Reinforcement Learning with Deep-Q-Network (DQN)\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to implement a Deep-Q-Network that is able to do a classic control. The approaches are build upon the paper by DeepMind: Playing Atari with Deep Reinforcement Learning [paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), which first introduces the notion of a Deep Q-Network.\n",
    "<img src=\"graphics/atari_play.png\" width=\"700\"><br>\n",
    "<center> Fig. 1: Breakout environment of the Atari game </center>\n",
    "\n",
    "## Core idea\n",
    "As you probably remember from the lecture, during trial and error we can learn a policy for our Atari game, and model it within our Q-matrix. This is done with a deep neural network. After training, this Q-matrix gives us an estimate of the expected reward when taking action a in state s: Q(s, a).\n",
    "Playing the action with the maximum Q-value in any given state is the same as playing optimal, or following a full exploitation strategy.\n",
    "\n",
    "## OpenAI Gym\n",
    "[OpenAI Gym](https://gym.openai.com/docs/) is a library that can simulate a large number of reinforcement learning environments, including Atari games (these need to be installed additionaly). You will need Python 3.5+\n",
    "\n",
    ">pip install gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking our cart pole on a first ride\n",
    "Now that you have gym installed you can load the 'Pendulum-v0' environment of Atari.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gym module\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment\n",
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "# Reset, it returns the starting frame\n",
    "frame = env.reset()\n",
    "\n",
    "for _ in range(100000):\n",
    "    # Perform a random action, returns the new frame, reward and whether the game is over\n",
    "    \n",
    "    '''\n",
    "    Implement to sample a random action from the action space within the loaded environment\n",
    "    action = environment.action_space.sample()\n",
    "    '''\n",
    "    \n",
    "    observation, reward, is_done, info = env.step(action)\n",
    "    print('observation: ', observation, 'reward: ', reward)\n",
    "    if is_done: break\n",
    "    env.render()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already looks nice, yet the actions are random and thus it is time to better understand our environment. And to implement our Deep-Q-Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "The observation is made up of cos(theta), sin(theta) and theta dot. \n",
    "Theta is normalized between -pi and pi.\n",
    "\n",
    "## Action\n",
    "Joint effort -2.0 to +2.0\n",
    "Write a function to discretize the continuous action space of the joint effort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the action space\n",
    "def create_action_bins(num_action_bins):\n",
    "    '''\n",
    "    Using linspace of numpy implement the action bins for the pendulum, when given the number of the action bins as argument\n",
    "    actionbins = \n",
    "    '''\n",
    "    \n",
    "    return actionbins\n",
    "\n",
    "# depending on the action, find the according actionbin \n",
    "# discretization of the continuous action space\n",
    "def find_actionbin(action, actionbins):\n",
    "    idx = (np.abs(actionbins - action)).argmin()\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward\n",
    "The reward is defined as\n",
    "> -(theta^2 + 0.1 x theta_dt^2 + 0.001 x action^2)\n",
    "\n",
    "What is the lowest expected cost? And what is the highest cost?\n",
    "\n",
    "-(pi^2 + 0.1 x 8^2 + 0.001 x 2^2) = -16.2736044\n",
    "\n",
    "-(0^2 + 0.1 x 0^2 + 0.001 x 0^2) = 0\n",
    "\n",
    "From this reward function, what is the goal of the agent?\n",
    "In essence, the goal is to remain at zero angle (vertical), with the least rotational velocity, and the least effort.\n",
    "\n",
    "For a hint have a look at the [wiki](https://github.com/openai/gym/wiki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(memory, gamma=0.9):\n",
    "    for state, action, reward, state_new in memory:\n",
    "        \n",
    "        # flatten state to make it compatible to our neural network\n",
    "        flat_state_new = np.reshape(state_new, [1, 3])\n",
    "        flat_state = np.reshape(state, [1, 3])\n",
    "\n",
    "        # determine estimated reward given state s' after action a, \n",
    "        # combination of observed and predicted exploited reward.\n",
    "        '''\n",
    "        Implement the Q-function for the flat_state_new\n",
    "        target = \n",
    "        '''\n",
    "        \n",
    "        # determine current expected agent rewards\n",
    "        targetfull = model.predict(flat_state)\n",
    "        \n",
    "        # update current expected rewards with the emulated prediced reward\n",
    "        targetfull[0][action] = target\n",
    "        \n",
    "        # Fit model based on emulation and prediction\n",
    "        model.fit(flat_state, targetfull, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Model\n",
    "\n",
    "As a reminder, this is our Q function.\n",
    "> Q(s, a) = r + gamma max_a'(Q(s, a'))\n",
    "\n",
    "The input of our neural network, our generalizable Q-matrix, will be the observation or the state of the pendulum. \n",
    "and the output will be the estimate of the reward taking the action a'. Gamma is the discount factor of the predicted reward in our next state. r is the reward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first network we will implement a DQN with keras:\n",
    "\n",
    "- Layer with 128 ReLU units\n",
    "- Layer with 64 ReLU units\n",
    "- 3 inputs and one output per action bin with linear activation function\n",
    "- Adam optimizer with learning rate 0.0002, beta_1 0.9 and beta_2 0.999\n",
    "- Loss mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Deep-Q-Network in keras\n",
    "\n",
    "def build_model(num_output_nodes):\n",
    "    model = Sequential()\n",
    "\n",
    "    '''\n",
    "    Design your model here (find description above)\n",
    "    model. \n",
    "    model.\n",
    "    model.\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    Define the optimizer (find description above)\n",
    "    adam = optimizers.\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    define the loss (hint: search the web for keras loss and see description above)\n",
    "    model.compile(loss=, optimizer=adam)\n",
    "    '''\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(epsilon, gamma, training_iterations, sequence_iterations):\n",
    "    \n",
    "    # These are hyperparameters to play around with, after your first run.\n",
    "    epsilon_decay = 0.9999\n",
    "    epsilon_min = 0.02\n",
    "    steps_per_sequence = 250\n",
    "\n",
    "    for epoch in range(0, training_iterations // sequence_iterations):\n",
    "        for sequence_id in range(0, sequence_iterations):\n",
    "            state = env.reset()\n",
    "            memory = deque()\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            # Easy implementation of decaying exploration\n",
    "            '''\n",
    "            Given epsilon, epsilon_decay and epsilon_min implement a simple method for a decaying epsilon.\n",
    "            \n",
    "            Decay epsilon here, while bigger than epsilon_min\n",
    "            if \n",
    "                            \n",
    "            '''\n",
    "            \n",
    "            for i in range(0, steps_per_sequence):\n",
    "                    \n",
    "                '''\n",
    "                Given epsilon implement a simple method for trading off exploration and exploitation\n",
    "            \n",
    "                Hint: For random values (use numpy) smaller than epsilon we want to explore\n",
    "                \n",
    "                if \n",
    "                    action = \n",
    "                \n",
    "                Hint: For random values larger than epsilon we want to exploit\n",
    "                \n",
    "                else:\n",
    "                    flat_state = np.reshape(state, [1, 3])\n",
    "                    action = \n",
    "                '''\n",
    "\n",
    "                # determine action\n",
    "                actionbin = find_actionbin(action, actionbinslist)\n",
    "                action = actionbinslist[actionbin]\n",
    "                action = np.array([action])\n",
    "\n",
    "                # emulate the action in the simulation and observe the transition \n",
    "                # as well as the reward\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                state_new = observation\n",
    "\n",
    "                '''\n",
    "                save transitions into memory\n",
    "                Hint: The memory is used as an argument for the train_model function.\n",
    "                \n",
    "                memory.append((_, _, _, _))\n",
    "                \n",
    "                '''\n",
    "                \n",
    "                state = state_new\n",
    "                \n",
    "            # train model on the samples memories\n",
    "            train_model(memory, gamma)\n",
    "            \n",
    "            print(epoch , ' epoch', sequence_id, ' sequence. Average reward = ', total_reward / steps_per_sequence, '. epsilon = ', epsilon)\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for running the policy of our DQN after loading or training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(rounds):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "\n",
    "    for _ in range(0, rounds):\n",
    "        # Rendering for visualization\n",
    "        env.render()\n",
    "\n",
    "        flat_state = np.reshape(state, [1, 3])\n",
    "        actionbin = np.argmax(model.predict(flat_state))\n",
    "\n",
    "        action = actionbinslist[actionbin]\n",
    "        action = np.array([action])\n",
    "\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        totalreward += reward\n",
    "\n",
    "        state_new = observation\n",
    "        state = state_new\n",
    "        \n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "# These are hyperparameters to play around with\n",
    "\n",
    "# iterations\n",
    "training_iterations = 1000\n",
    "sequence_iterations = 25\n",
    "\n",
    "# epsilon (setting exploitation vs exploration)\n",
    "epsilon = 1\n",
    "\n",
    "# gamma (importance of predicted estimated reward)\n",
    "gamma = 0.9\n",
    "\n",
    "# Discretization settings for the action space\n",
    "num_action_bins = 20\n",
    "actionbinslist = create_action_bins(num_action_bins)\n",
    "\n",
    "\n",
    "\n",
    "# Build model\n",
    "model = build_model(num_action_bins)\n",
    "\n",
    "run_episodes(epsilon, gamma, training_iterations, sequence_iterations)\n",
    "\n",
    "'''\n",
    "training takes super long, this is not efficient at all, how can we bypass this?\n",
    "Hint: See cells in run pretrained model.\n",
    "\n",
    "'''\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save model weights\n",
    "\n",
    "print('saving model')\n",
    "model.save('pendulum_model_juno_' + str(training_iterations) + '.h5')\n",
    "print('model saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on 10 test runs with 100 steps each\n",
    "trarray = []\n",
    "rounds = 100\n",
    "for i in range(10):\n",
    "    trarray.append(play_game(rounds))\n",
    "    print(i, ' sequence. Average test reward = ', np.average(trarray)/rounds, 'Average test reward = ', trarray[-1]/rounds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pretrained model\n",
    "\n",
    "In case you already trained a model or want to load the pretrained model for sanity checking use the following script (make sure you executed the necessary cells starting with the imports).\n",
    "\n",
    "- How does the performance change with the amount of trained iterations?\n",
    "- How can we measure performance to begin with?\n",
    "- Is it sufficient to start the play_game function a single time? \n",
    "- How can we make sure, that the evaluation is meaningful?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "-\n",
    "-\n",
    "-\n",
    "-\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "actionbinslist = create_action_bins(20)\n",
    "\n",
    "# 'pendulum_model_[iterationstrained].h5' \n",
    "# iterationstrained: 100, 1000, 10000\n",
    "model = load_model('pendulum_model_1000.h5')\n",
    "\n",
    "'''\n",
    "Is the next line meaningful for evaluation, if not, what can we do?\n",
    "\n",
    "play_game(rounds=250)\n",
    "'''    \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- Implement a skip frame approach\n",
    "- Experiment with the discretization of the action bins (e.g. advantages and disadvantages of triadisation)\n",
    "- Experiment with exploration vs exploitation\n",
    "\n",
    "Send extended ipynb file to mark.schutera@kit.edu for the chance to get bonus points for the final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
