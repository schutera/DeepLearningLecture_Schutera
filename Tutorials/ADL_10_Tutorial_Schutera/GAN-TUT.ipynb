{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "\n",
    "contact: Mark.schutera@kit.edu\n",
    "\n",
    "\n",
    "# Generative Adversarial Neural Networks (GANs)\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to implement a Generative Adversarial Neural Network for image generation on the MNIST dataset. Labeled training data is the fuel for supervised machine learning approaches. Thus GANs can be seen as the holy grail when it comes to generating additional data affiliated with a given source domain. GANs have first been introduced by [Goodfellow et al.](http://arxiv.org/abs/1406.2661)\n",
    "\n",
    "<img src=\"graphics/stormtrooper.jpg\" width=\"700\"><br>\n",
    "<center> Fig. 1: Stormtrooper reflects the idea of generating multiple instances of the same source domain. </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "GANs are based on a joint training of at least two separate neural networks. The generator and the discriminator.\n",
    "The generator takes an input, most often a latent random input, and maps this input to an output with the dimensions of a source domain sample, meaning real-world data. The discriminator takes inputs from either the source domain or the generator. The discriminator then does a binary classification, learning to distinguish between generated and real samples.\n",
    "\n",
    "<img src=\"graphics/GANarchitecture.jpg\" width=\"700\"><br>\n",
    "<center> Fig. 2: Overview of a simple GAN architecture with discriminator and generator. </center>\n",
    "\n",
    "The training process happens in two passes. During the first pass the generator is frozen, and the discriminator is trained to distinguish between real and generated samples. \n",
    "In the second pass, the discriminator is frozen and the generator is trained using the discriminator classification as objective, backpropagating the error. \n",
    "By iteratively repeating those two passes, the quality of the generated samples increases, while the ability of the discriminator of distinguishing between real and generated improves too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your GAN\n",
    "\n",
    "Do the necessary imports.\n",
    "And load the [MNIST](http://yann.lecun.com/exdb/mnist/) data set - the hello world for machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"C:\\\\Users\\\\Z624284\\\\Desktop\\\\VL_Weingarten\\\\10 Block GenerativeAdversarialNetworks\\\\ADL_08_Tutorial_Schutera\\\\data\\\\\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up dimensions and initialization\n",
    "\n",
    "- Determine and define the image dimension, which for MNIST, as we know by now, is 28x28 Pixels, this is the number of inputs to the discriminator.\n",
    "- The input to the generator will be a random latent variable, the more inputs and thus combinations we allow here, the more variance our generated samples will show.\n",
    "- Next we will define the dimension (number of neurons) of our hidden layer for the generator network and the discriminator network. Both have one hidden layer, and should be approximately equally complex to keep the balance between the two networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Params\n",
    "image_dim = 784 # 28*28 pixels\n",
    "gen_hidden_dim = 256\n",
    "disc_hidden_dim = '''define amount of units for the discriminator '''\n",
    "noise_dim = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders for the inputs\n",
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name='input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name='disc_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot initialization\n",
    "\n",
    "We will initialize our weight with the Glorot initialization which was presented in this [paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot_init(shape):\n",
    "    return tf.'''use the tensorflow random utility'''(shape=shape, stddev='''research the equation for the Glorot initializer ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up parameters\n",
    "Initialize the weights with the Glorot initializer. And initialize the biases as zeroes.\n",
    "\n",
    "##### Why can we initialize the bias as zeroes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias for discriminator and generator\n",
    "weights = {\n",
    "    'gen_hidden1': tf.Variable('''glorot initialization'''),\n",
    "    'gen_out': tf.Variable('''glorot initialization'''),\n",
    "    \n",
    "    'disc_hidden1': tf.Variable('''glorot initialization'''),\n",
    "    'disc_out': tf.Variable('''glorot initialization'''),\n",
    "}\n",
    "biases = {\n",
    "    'gen_hidden1': tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
    "    \n",
    "    'disc_hidden1': tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc_out': tf.Variable(tf.zeros([1])),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Generator\n",
    "\n",
    "The Generator will be designed as a Fully Connected Neural Network (FCN). Starting from our input x implement the following operations: \n",
    "- first we multiply the inputs (first layer) with the weights of the hidden layer.\n",
    "- add the biases.\n",
    "- map the output with the ReLU activation function.\n",
    "- mulitply the output of the hidden layer with the weights of the output layer.\n",
    "- add the biases.\n",
    "- map the output with the Sigmoid activation function.\n",
    "\n",
    "###### Why is the activation function of the output layer realized as sigmoid function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generator(x):\n",
    "    hidden_layer = tf.'''multiplication'''\n",
    "    hidden_layer = tf.'''addition'''\n",
    "    hidden_layer = tf.nn.'''activation'''\n",
    "    out_layer = tf.'''multiplication'''\n",
    "    out_layer = tf.'''addition'''\n",
    "    out_layer = tf.nn.'''activation'''\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Discriminator\n",
    "\n",
    "The discriminator will be designed similar to the generator: \n",
    "- first we multiply the inputs (first layer) with the weights of the hidden layer.\n",
    "- add the biases.\n",
    "- map the output with the ReLU activation function.\n",
    "- mulitply the output of the hidden layer with the weights of the output layer.\n",
    "- add the biases.\n",
    "- map the output with the Sigmoid activation function.\n",
    "\n",
    "###### Why is the activation function of the output layer realized as sigmoid function?\n",
    "\n",
    "We need to map our output to a range of values between 0 and 1 or source or generated for binary classification. This nicely is reflected by the range of values of the sigmoid function.\n",
    "\n",
    "###### Why is the discriminator architecture so similar to the generator? Discuss complexity of the two models with respect to their task.\n",
    "\n",
    "We need both models to be similar complex to make an iterative interplay possible. If one model would have a peak in performance improvement it might happen, that the other model wouldn't be able to benefit from the feedback of the other model anymore. Thus the complexity of the models should be chosen similar. \n",
    "Yet the discriminator has a slightly easier task, doing a binary classification - it is thus adviced to sometimes handicap this model by limitting the complexity or the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def discriminator(x):\n",
    "'''design discriminator similar to the generator architecture'''\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator Training\n",
    "\n",
    "The discriminator is trained on both, samples from source domain and from the generated domain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipelines for the discriminator network inference\n",
    "\n",
    "# sample from source\n",
    "disc_real = discriminator(disc_input)\n",
    "\n",
    "# sample from generator\n",
    "disc_fake = discriminator('''generator sample''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function - Min Max Game\n",
    "\n",
    "The loss for the discriminator is as follows:\n",
    "\\begin{align}\n",
    "    -\\log_{10}(discriminator_{real})-\\log_{10}(1-discriminator_{fake})\n",
    "\\end{align}\n",
    "\n",
    "For better understanding lets assume a generated sample. When feeding the generated sample into the discriminator and the discriminator is able to detect the domain, the sigmoid output will be a value around 0. The loss should thus be low in this case. We achieve this by capsuling the discriminator output as: \n",
    "\n",
    "$\\log_{10}(1-discriminator_{fake})$.\n",
    "\n",
    "- If the generated image is detected as such, the loss will be $-\\log(1-(0))=-\\log(1)$ and thus tending to become 0. \n",
    "- If the generated image is detected as source image, the loss will be $-\\log(1-(1))=-\\log(0)$ and thus tending to become inifite large. \n",
    "\n",
    "##### Elaborate the loss for a real image given to the discriminator\n",
    "- If the source image is detected as such, '''....''' \n",
    "- If the source image is detected as generated image, '''....'''\n",
    "\n",
    "##### Elaborate the loss for the generator\n",
    "- If the generated image is detected as such, '''....'''\n",
    "- If the generated image is detected as source image, '''....'''\n",
    "\n",
    "The loss is determined over multiple samples, thus the mean output of the models is considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Loss\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "disc_loss = '''implement the loss for the discriminator'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Training and Optimizer\n",
    "\n",
    "Set up the optimizer as Adam optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Optimizers\n",
    "\n",
    "learning_rate = 0.0002\n",
    "\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Pass Training\n",
    "\n",
    "In Tensorflow by default each optimizer updateds all variables, but in this special architecture we need either two update on the generator or the discriminator only. \n",
    "\n",
    "Therefore we define the two passes as separate optimization tasks, while defining the according variables that are affiliated to the pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "            biases['gen_hidden1'], biases['gen_out']]\n",
    "disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
    "            biases['disc_hidden1'], biases['disc_out']]\n",
    "\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list='''hand over weights of the generator only''')\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list='''hand over weights of the discriminator only''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Params (feel free to mess around with the hyperparameters)\n",
    "num_steps = 70000\n",
    "batch_size = 128\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "g_loss = []\n",
    "d_loss = []\n",
    "\n",
    "for i in range(1, num_steps+1):\n",
    "    \n",
    "    # Get batch from MNIST dataset\n",
    "    batch_x, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    # Generate latent random variable to feed to the generator, by drawing from a uniform distribution \n",
    "    z = np.random.uniform(-1., 1., size=[batch_size, noise_dim])\n",
    "\n",
    "    # Train\n",
    "    feed_dict = {disc_input: batch_x, gen_input: z}\n",
    "    _, _, gl, dl = sess.run([\n",
    "                             train_gen, \n",
    "                             train_disc, \n",
    "                             gen_loss, \n",
    "                             disc_loss\n",
    "                            ],\n",
    "                            feed_dict=feed_dict)\n",
    "\n",
    "    g_loss.append(gl)\n",
    "    d_loss.append(dl)\n",
    "\n",
    "    if i % 2000 == 0 or i == 1:\n",
    "        print('Step %i: Generator Loss: %f, Discriminator Loss: %f' % (i, gl, dl))\n",
    "        \n",
    "    print('Training finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot visualization of the losses, you should pay close attention.\n",
    "plt.plot(g_loss, label='Loss of the Generator')\n",
    "plt.plot(d_loss, label='Loss of the Discriminator')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss visualization tells us a lot about the training process. \n",
    "- Make sure the discriminator does have a continuous loss\n",
    "- The generator loss should continuously decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why do we only load the MNIST images and not the groundtruth? \n",
    "The GAN does not care for class labels, and the domain labels are already given by the pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking the Generator for a ride.\n",
    "\n",
    "After training the generator, we do a visual check of the generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Generate images from a latent random variable, using the generator network.\n",
    "# first we will create an empty np.array with the size of 28x28 \n",
    "canvas = np.empty((28, 28))\n",
    "\n",
    "# We will generate the latent random variable drawing from the uniform distribution in [-1, 1] as a 1x100 tensor\n",
    "z = np.random.uniform(-1., 1., size=[1, noise_dim])\n",
    "\n",
    "# we will now feed z as input to the generator\n",
    "g = sess.run(generator(gen_input), feed_dict={gen_input: z})\n",
    "\n",
    "# The generated sample is reshaped and visualized.\n",
    "canvas[0:28, 0:28] = g[0].reshape([28, 28])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(canvas, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n*n images from noise, using the generator network.\n",
    "n = 15\n",
    "canvas = np.empty((28 * n, 28 * n))\n",
    "for i in range(n):\n",
    "    z = np.random.uniform(-1., 1., size=[n, noise_dim])\n",
    "    # Generate image from noise.\n",
    "    g = sess.run(generator(gen_input), feed_dict={gen_input: z})\n",
    "\n",
    "    for j in range(n):\n",
    "        # Draw the generated digits\n",
    "        canvas[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = g[j].reshape([28, 28])\n",
    "\n",
    "plt.figure(figsize=(n, n))\n",
    "plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- We can notice that some digits are generated more frequent than others, why is that. Implement an approach that does solve this issue or at least weaken the effect.\n",
    "- So far we have only evaluated the performance of the generator qualitatively. How could a quantitative evaluation look like? Implement a quantitative evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
